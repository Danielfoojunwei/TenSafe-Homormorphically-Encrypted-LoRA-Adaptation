# TenSafe SFT Training Configuration
#
# This configuration file defines settings for supervised fine-tuning (SFT)
# with pluggable loss functions.
#
# Usage:
#   python scripts/train.py --config configs/train_sft.yaml

# Training mode: sft (supervised fine-tuning) or rlvr (reinforcement learning)
training:
  mode: sft
  seed: 42
  deterministic: false
  max_steps: 1000
  eval_steps: 100
  save_steps: 500
  logging_steps: 10

# Model configuration
model:
  ref: "meta-llama/Llama-3-8B"
  trust_remote_code: true

# LoRA configuration (based on "LoRA Without Regret" best practices)
# Key findings:
#   1. Apply to ALL layers (MLP + Attention) - critical for matching full fine-tuning
#   2. Alpha = 2 * rank is optimal
#   3. Use rsLoRA scaling for high ranks (>32)
#   4. Learning rate should be ~10x full fine-tuning rate
lora:
  rank: 32
  alpha: 64.0  # 2 * rank (research-backed optimal)
  dropout: 0.0  # Set to 0 for faster training; research shows minimal benefit
  target_modules:
    # CRITICAL: Apply to ALL linear layers, not just attention
    # "LoRA Without Regret" shows MLP layers are as important as attention
    # Attention layers
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    # MLP layers (essential for matching full fine-tuning performance)
    - gate_proj
    - up_proj
    - down_proj
  bias: none
  # Advanced options (PEFT library)
  use_rslora: true  # Rank-stabilized scaling (α/√r instead of α/r)
  use_dora: false   # Enable for DoRA weight decomposition
  # LoRA+ optimization (different learning rates for A and B matrices)
  lora_plus:
    enabled: true
    ratio: 16.0  # lr_B = ratio * lr_A (recommended: 8-16)

# Optimizer configuration
# Note: LoRA optimal LR is ~10x higher than full fine-tuning (from research)
optimizer:
  name: adamw
  learning_rate: 2.0e-4  # 10x typical full fine-tuning rate (research-backed)
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

# Learning rate scheduler
scheduler:
  type: cosine
  warmup_ratio: 0.03
  min_lr_ratio: 0.0

# Loss function configuration
# Can be:
#   - A built-in name: "token_ce", "margin_ranking", "contrastive", "mse"
#   - A dotted path: "my_module.losses:custom_loss"
loss:
  type: token_ce
  kwargs:
    ignore_index: -100
    label_smoothing: 0.0
    reduction: mean

# Batch configuration
batch:
  size: 8
  gradient_accumulation_steps: 4
  max_seq_length: 2048

# Differential privacy (optional)
dp:
  enabled: false
  noise_multiplier: 1.0
  max_grad_norm: 1.0
  target_epsilon: 8.0
  target_delta: 1.0e-5
  accountant_type: rdp

# Checkpointing
checkpoint:
  save_total_limit: 3
  resume_from: null  # Path to checkpoint to resume from

# Data configuration
data:
  train_path: null  # Path to training data
  eval_path: null   # Path to evaluation data
  dataset_name: null  # HuggingFace dataset name
  text_field: text  # Field containing text data

# Output configuration
output:
  dir: outputs/sft
  experiment_name: null  # Auto-generated if null

# Hardware configuration
hardware:
  bf16: true
  fp16: false
  tf32: true
  use_flash_attention: true

# Logging configuration
logging:
  level: INFO
  log_to_file: true
  report_to: []  # Empty for no external reporting

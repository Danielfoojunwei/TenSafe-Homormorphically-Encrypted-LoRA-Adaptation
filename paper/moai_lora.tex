% MOAI-LoRA: Rotation-Free Homomorphic Encryption for Privacy-Preserving LLM Personalization
% Target venue: NeurIPS 2026 / ICML 2026
% REVISED VERSION with realistic benchmarks and honest comparison

\documentclass{article}

% Required packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,fit,backgrounds}

% Page setup (NeurIPS style approximation)
\usepackage[margin=1in]{geometry}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}

% Custom commands
\newcommand{\ie}{\textit{i.e.}}
\newcommand{\eg}{\textit{e.g.}}
\newcommand{\etc}{\textit{etc.}}
\newcommand{\etal}{\textit{et al.}}
\newcommand{\ct}{\mathsf{ct}}
\newcommand{\pt}{\mathsf{pt}}
\newcommand{\Enc}{\mathsf{Enc}}
\newcommand{\Dec}{\mathsf{Dec}}
\newcommand{\Rot}{\mathsf{Rot}}
\newcommand{\Rescale}{\mathsf{Rescale}}
\newcommand{\PCMM}{\mathsf{PCMM}}
\newcommand{\CCMM}{\mathsf{CCMM}}

\title{
\textbf{MOAI-LoRA: Rotation-Free Homomorphic Encryption\\for Privacy-Preserving LLM Personalization}
}

\author{
Anonymous Authors\\
Anonymous Institution\\
\texttt{anonymous@institution.edu}
}

\date{}

\begin{document}

\maketitle

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
Low-Rank Adaptation (LoRA) has emerged as the dominant method for personalizing Large Language Models (LLMs), but deploying personalized adapters raises privacy concerns---adapter weights can leak training data, and user queries expose private information. We present \textbf{MOAI-LoRA}, a rotation-free algorithm for encrypted LoRA inference using the CKKS homomorphic encryption scheme. Our key insight is that LoRA's low-rank structure, combined with the ciphertext-plaintext (Ct$\times$Pt) computation regime, enables column-packed matrix multiplication that \emph{eliminates rotation operations entirely} for single-block computation. Rotations are the dominant cost in CKKS ($\sim$24$\times$ slower than basic operations), so their elimination provides substantial algorithmic improvement. We formally prove that MOAI-LoRA maintains IND-CPA security while achieving: (1) \textbf{zero rotations} for LoRA layers where hidden dimension fits in one slot block, (2) only \textbf{$\log_2(b)$ rotations} for $b$-block configurations (vs. $O(n)$ in naive approaches), and (3) \textbf{depth-2} multiplicative depth (minimal for two matrix multiplications). Our evaluation across Llama-2, Llama-3, Mistral, and GPT-J architectures shows MOAI-LoRA reduces rotation count by \textbf{2000$\times$+} compared to diagonal-method baselines. With aggressive optimization (rank-8, selective projections, batch-32, GPU acceleration), we achieve \textbf{0.5 s/tok} (2 tok/s) for Llama-scale models---a \textbf{3000$\times$} improvement over naive baselines. We provide the first systematic comparison against SHE-LoRA, PrivTuner, and the Encryption-Friendly LLM architecture.
\end{abstract}

%==============================================================================
% 1. INTRODUCTION
%==============================================================================
\section{Introduction}

Large Language Models (LLMs) have demonstrated remarkable capabilities, and Low-Rank Adaptation (LoRA)~\cite{hu2021lora} has become the standard for efficient personalization. However, deploying personalized LoRA adapters introduces critical privacy concerns: adapter weights trained on sensitive data can leak information~\cite{carlini2021extracting}, and users submitting queries expose their inputs to model servers.

Homomorphic encryption (HE) offers a principled solution by enabling computation on encrypted data. The CKKS scheme~\cite{cheon2017homomorphic} supports approximate arithmetic on encrypted real numbers, but HE-based inference has been impractical due to prohibitive costs. Recent work on encryption-friendly architectures~\cite{encfriendly2024} shows that even with optimizations, encrypted BERT inference takes \textbf{25+ seconds} for a 2-layer model.

\paragraph{The Rotation Bottleneck.} In CKKS, the most expensive operation is \emph{rotation} (also called slot permutation), which requires key-switching. Prior work~\cite{encfriendly2024} reports rotations are \textbf{24$\times$ slower} than basic ciphertext-plaintext multiplication. Standard matrix-vector multiplication using the diagonal method~\cite{halevi2014algorithms} requires $O(n)$ rotations for dimension $n$, making it impractical for LLM hidden dimensions ($n \geq 768$).

\paragraph{Our Key Insight.} We observe that LoRA's structure enables a fundamentally more efficient approach. The LoRA delta $\Delta y = \alpha \cdot B(Ax)$ operates in the \emph{ciphertext-plaintext} (Ct$\times$Pt) regime: user activations $x$ are encrypted, while adapter weights $A, B$ are plaintext (owned by the server). This asymmetry is crucial: plaintext values can be \emph{arbitrarily rearranged} during encoding to match the ciphertext's SIMD slot layout, eliminating rotations.

\paragraph{Contributions.}
\begin{enumerate}
    \item \textbf{MOAI-LoRA Algorithm} (\S\ref{sec:design}): Column-packed matrix multiplication achieving \emph{zero rotations} for single-block LoRA, and $O(\log b)$ rotations for $b$-block configurations.

    \item \textbf{Security Analysis} (\S\ref{sec:security}): Formal proof that MOAI-LoRA maintains IND-CPA security of the underlying CKKS scheme.

    \item \textbf{Multi-Architecture Evaluation} (\S\ref{sec:evaluation}): Systematic comparison across Llama-2-7B, Llama-3-8B, Mistral-7B, and GPT-J-6B, with rotation count reductions of 47-512$\times$.

    \item \textbf{Ablation Study} (\S\ref{sec:ablation}): Isolating contributions of column packing, depth optimization, and block partitioning.

    \item \textbf{Baseline Comparison} (\S\ref{sec:baselines}): First systematic comparison against SHE-LoRA~\cite{shelora2025}, PrivTuner~\cite{privtuner2024}, and Encryption-Friendly LLM~\cite{encfriendly2024}.
\end{enumerate}

%==============================================================================
% 2. BACKGROUND AND RELATED WORK
%==============================================================================
\section{Background and Related Work}

\subsection{Low-Rank Adaptation (LoRA)}

LoRA~\cite{hu2021lora} adapts a pre-trained weight matrix $W_0 \in \mathbb{R}^{d \times k}$ by learning a low-rank update:
\begin{equation}
    W = W_0 + \Delta W = W_0 + \frac{\alpha}{r} BA
\end{equation}
where $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$, $r \ll \min(d, k)$, and $\alpha$ is a scaling factor. For input $x$:
\begin{equation}
    y = W_0 x + \frac{\alpha}{r} B(Ax) = W_0 x + \Delta y
    \label{eq:lora}
\end{equation}

\subsection{CKKS Homomorphic Encryption}

The CKKS scheme~\cite{cheon2017homomorphic} encrypts vectors $\mathbf{m} \in \mathbb{C}^{N/2}$ (or $\mathbb{R}^{N}$) into polynomial ciphertexts with SIMD operations:

\begin{itemize}
    \item \textbf{Ct$\times$Pt Multiply}: $\Enc(a) \cdot \pt(b) = \Enc(a \odot b)$
    \item \textbf{Rotation}: $\Rot_k(\Enc(\mathbf{a}))$ cyclically shifts slots by $k$
    \item \textbf{Rescale}: Reduces scale after multiplication to manage noise
\end{itemize}

\paragraph{Cost Model.} Based on~\cite{encfriendly2024}, relative costs on GPU are:
\begin{center}
\begin{tabular}{lcc}
\toprule
Operation & Relative Cost & Absolute (ms)$^\dagger$ \\
\midrule
Ct + Ct & 1$\times$ & $\sim$10 \\
Ct $\times$ Pt & 6$\times$ & $\sim$60 \\
\textbf{Rotation} & \textbf{24$\times$} & $\sim$\textbf{240} \\
Rescale & 2$\times$ & $\sim$20 \\
\bottomrule
\end{tabular}
\end{center}
{\small $^\dagger$Estimated from~\cite{encfriendly2024} PCMM cost of 275ms for $(768 \times 768)$ matrix.}

\subsection{Related Work}

\paragraph{SHE-LoRA}~\cite{shelora2025} uses selective homomorphic encryption in federated learning, encrypting only sensitive columns of LoRA matrices. It reduces communication by 94.9\% and encryption time by 99.8\% compared to full encryption, but does not address rotation optimization during computation.

\paragraph{Private LoRA Fine-tuning}~\cite{privatelora2025} presents an interactive protocol for HE-based LoRA training on Llama-3.2-1B using the Concrete ML library with 8-bit quantization. They demonstrate convergence but do not report per-token inference latency.

\paragraph{Encryption-Friendly LLM}~\cite{encfriendly2024} (ICLR 2025) combines LoRA with Gaussian kernels to avoid expensive ciphertext-ciphertext matmul and softmax. They report 25.78s inference for a 2-layer BERT, achieving 2.3$\times$ speedup over baselines.

\paragraph{PrivTuner}~\cite{privtuner2024} integrates FHE with LoRA for privacy-preserving parameter-efficient fine-tuning using CKKS.

\paragraph{MOAI}~\cite{moai2025} introduces rotation-minimal matrix multiplication for general HE inference. We extend MOAI specifically for LoRA, exploiting its low-rank structure for further optimization.

%==============================================================================
% 3. MOAI-LORA DESIGN
%==============================================================================
\section{MOAI-LoRA Design}
\label{sec:design}

\subsection{Threat Model}

We consider a semi-honest model server that owns the base model $W_0$ and LoRA adapters $(A, B)$, receives encrypted queries $\Enc(x)$, and returns encrypted outputs $\Enc(y)$. The user holds the secret key. Neither party learns the other's private data.

\subsection{The Rotation Problem in Standard HE Matrix Multiplication}

Standard encrypted matrix-vector multiplication $y = Mx$ for $M \in \mathbb{R}^{n \times n}$ uses the diagonal method:
\begin{equation}
    y = \sum_{i=0}^{n-1} \text{diag}_i(M) \odot \Rot_i(\Enc(x))
\end{equation}
This requires $n-1$ rotations. For LLM hidden dimensions:

\begin{center}
\begin{tabular}{lcc}
\toprule
Model & Hidden Dim & Rotations (Diagonal) \\
\midrule
GPT-2 & 768 & 767 \\
Llama-2-7B & 4096 & 4095 \\
Llama-3-8B & 4096 & 4095 \\
Mistral-7B & 4096 & 4095 \\
\bottomrule
\end{tabular}
\end{center}

At 240ms per rotation, this yields \textbf{983 seconds} ($>$16 minutes) per layer for Llama-scale models---clearly impractical.

\subsection{MOAI Column-Packed Matrix Multiplication}

Our key insight: in the Ct$\times$Pt regime, we can encode plaintext weights to align with ciphertext slot layout, eliminating rotations.

\begin{definition}[Batch-First Slot Layout]
For batch size $B$ and hidden dimension $h$, we pack activations as:
\begin{equation}
    \text{slot}[b + B \cdot c] = x^{(b)}_c
\end{equation}
where $b \in [0, B)$ is batch index and $c \in [0, h)$ is channel index.
\end{definition}

\begin{definition}[Column Packing]
For weight matrix $M \in \mathbb{R}^{d \times h}$, we encode column $j$ as:
\begin{equation}
    \text{Encode}(M_{:,j}) = [M_{0,j}]_{B}, [M_{1,j}]_{B}, \ldots, [M_{d-1,j}]_{B}
\end{equation}
where $[v]_B$ denotes $v$ repeated $B$ times.
\end{definition}

\begin{theorem}[Rotation-Free Ct$\times$Pt MatMul]
\label{thm:rotfree}
For $y = Mx$ with $\Enc(x)$ in batch-first layout and $M$ column-packed:
\begin{equation}
    \Enc(y) = \sum_{j=0}^{h-1} \Enc(x)_j \odot \text{Encode}(M_{:,j})
\end{equation}
requires \textbf{zero rotations} when $h \cdot B \leq N$ (slot count).
\end{theorem}

\begin{proof}
Each term $\Enc(x)_j \odot \text{Encode}(M_{:,j})$ performs element-wise multiplication in aligned slots. The sum accumulates partial products without slot movement. Since all operations are slot-aligned, no rotations are needed. \qed
\end{proof}

\subsection{MOAI-LoRA Algorithm}

For LoRA delta computation $\Delta y = \frac{\alpha}{r} B(Ax)$:

\begin{algorithm}[h]
\caption{MOAI-LoRA Delta Computation}
\begin{algorithmic}[1]
\REQUIRE $\ct_x = \Enc(x)$, weights $A \in \mathbb{R}^{r \times h}$, $B \in \mathbb{R}^{d \times r}$
\ENSURE $\ct_\Delta = \Enc(\frac{\alpha}{r} B(Ax))$

\STATE $A_{\text{pack}} \gets \text{ColumnPack}(A)$
\STATE $B_{\text{pack}} \gets \text{ColumnPack}(\frac{\alpha}{r} \cdot B)$ \COMMENT{Fold scaling}

\STATE \textbf{// First matmul: $z = Ax$}
\STATE $\ct_z \gets \sum_{j=0}^{h-1} \ct_x[j] \odot A_{\text{pack}}[j]$
\STATE $\ct_z \gets \Rescale(\ct_z)$

\STATE \textbf{// Second matmul: $\Delta = Bz$}
\STATE $\ct_\Delta \gets \sum_{k=0}^{r-1} \ct_z[k] \odot B_{\text{pack}}[k]$
\STATE $\ct_\Delta \gets \Rescale(\ct_\Delta)$

\RETURN $\ct_\Delta$
\end{algorithmic}
\end{algorithm}

\paragraph{Complexity.}
\begin{itemize}
    \item \textbf{Rotations}: 0 (single block) or $O(\log b)$ ($b$ blocks)
    \item \textbf{Ct$\times$Pt muls}: $h + r$
    \item \textbf{Rescales}: 2
    \item \textbf{Multiplicative depth}: 2
\end{itemize}

\subsection{Multi-Block Extension}

When $h \cdot B > N$, we partition into blocks of size $s = \min(1024, 2^{\lfloor\log_2(N/2B)\rfloor})$:

\begin{equation}
    \text{num\_blocks} = b = \lceil h / s \rceil
\end{equation}

Cross-block aggregation requires a rotation tree with $\lceil \log_2 b \rceil$ rotations.

\begin{table}[h]
\centering
\caption{Block configurations for common LLM architectures ($N=16384$, $B=8$)}
\begin{tabular}{lccccc}
\toprule
Model & $h$ & Blocks & \multicolumn{2}{c}{Rotations} & Reduction \\
 & & & Diagonal & MOAI & \\
\midrule
BERT-base & 768 & 1 & 767 & \textbf{0} & $\infty$ \\
GPT-2 & 768 & 1 & 767 & \textbf{0} & $\infty$ \\
Llama-2-7B & 4096 & 4 & 4095 & \textbf{2} & 2047$\times$ \\
Llama-3-8B & 4096 & 4 & 4095 & \textbf{2} & 2047$\times$ \\
Mistral-7B & 4096 & 4 & 4095 & \textbf{2} & 2047$\times$ \\
GPT-J-6B & 4096 & 4 & 4095 & \textbf{2} & 2047$\times$ \\
Llama-2-70B & 8192 & 8 & 8191 & \textbf{3} & 2730$\times$ \\
\bottomrule
\end{tabular}
\label{tab:blocks}
\end{table}

%==============================================================================
% 4. SECURITY ANALYSIS
%==============================================================================
\section{Security Analysis}
\label{sec:security}

\begin{theorem}[IND-CPA Security of MOAI-LoRA]
\label{thm:security}
If CKKS is IND-CPA secure, then MOAI-LoRA is IND-CPA secure.
\end{theorem}

\begin{proof}
MOAI-LoRA performs the following operations on ciphertexts:
\begin{enumerate}
    \item Ct$\times$Pt multiplication: $\ct \cdot \pt$
    \item Ciphertext addition: $\ct_1 + \ct_2$
    \item Rescaling: $\Rescale(\ct)$
    \item (Multi-block only) Rotation: $\Rot_k(\ct)$
\end{enumerate}

All operations are standard CKKS operations that preserve IND-CPA security~\cite{cheon2017homomorphic}. Specifically:

\begin{itemize}
    \item \textbf{Ct$\times$Pt}: Multiplying by public plaintext does not leak information about the encrypted value beyond what the plaintext reveals.

    \item \textbf{Addition}: Homomorphic addition of IND-CPA secure ciphertexts yields an IND-CPA secure ciphertext.

    \item \textbf{Rescale}: A deterministic operation on the ciphertext that does not depend on the secret key.

    \item \textbf{Rotation}: Key-switching with evaluation keys does not compromise IND-CPA security when evaluation keys are generated correctly.
\end{itemize}

The column-packing strategy only affects how \emph{plaintexts} are encoded---it does not modify the ciphertext structure or introduce new operations. Therefore, MOAI-LoRA inherits the security of CKKS. \qed
\end{proof}

\begin{corollary}[128-bit Security]
With CKKS parameters $N=16384$ and $\log Q \leq 438$ bits, MOAI-LoRA achieves 128-bit security against known lattice attacks.
\end{corollary}

%==============================================================================
% 5. EVALUATION
%==============================================================================
\section{Evaluation}
\label{sec:evaluation}

We evaluate MOAI-LoRA on the following questions:
\begin{enumerate}
    \item How many rotations does MOAI-LoRA eliminate compared to baselines?
    \item What is the projected wall-clock speedup based on HE cost models?
    \item How does MOAI-LoRA compare to prior work (SHE-LoRA, PrivTuner, Enc-Friendly LLM)?
    \item What is the accuracy impact of CKKS approximation errors?
\end{enumerate}

\subsection{Experimental Setup}

\paragraph{CKKS Parameters.}
\begin{itemize}
    \item Polynomial degree: $N = 16384$ (8192 real slots)
    \item Coefficient modulus: [60, 40, 40, 60] bits (FAST) or [60, 45, 45, 45, 60] bits (SAFE)
    \item Scale: $2^{40}$ or $2^{45}$
    \item Security: 128-bit (lattice estimator~\cite{latticeestimator})
\end{itemize}

\paragraph{Model Architectures.}
\begin{center}
\begin{tabular}{lccccc}
\toprule
Model & Layers & $h$ & FFN & Heads & LoRA $r$ \\
\midrule
Llama-2-7B & 32 & 4096 & 11008 & 32 & 16 \\
Llama-3-8B & 32 & 4096 & 14336 & 32 & 16 \\
Mistral-7B & 32 & 4096 & 14336 & 32 & 16 \\
GPT-J-6B & 28 & 4096 & 16384 & 16 & 16 \\
BERT-large & 24 & 1024 & 4096 & 16 & 8 \\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Baselines.}
\begin{itemize}
    \item \textbf{Diagonal-CKKS}: Standard diagonal method~\cite{halevi2014algorithms}
    \item \textbf{OpenFHE-LoRA}: OpenFHE~\cite{openfhe} with naive matrix multiplication
    \item \textbf{SHE-LoRA}: Selective encryption approach~\cite{shelora2025}
    \item \textbf{Enc-Friendly}: LoRA + Gaussian kernel~\cite{encfriendly2024}
\end{itemize}

\subsection{Rotation Count Comparison}

\begin{table}[h]
\centering
\caption{Rotation count per LoRA layer (QKV projection)}
\begin{tabular}{lcccc}
\toprule
Model & Diagonal & OpenFHE & MOAI-LoRA & Reduction \\
\midrule
BERT-large & 1023 & 1023 & \textbf{0} & $\infty$ \\
Llama-2-7B & 4095 & 4095 & \textbf{2} & 2047$\times$ \\
Llama-3-8B & 4095 & 4095 & \textbf{2} & 2047$\times$ \\
Mistral-7B & 4095 & 4095 & \textbf{2} & 2047$\times$ \\
GPT-J-6B & 4095 & 4095 & \textbf{2} & 2047$\times$ \\
\bottomrule
\end{tabular}
\label{tab:rotations}
\end{table}

\subsection{Projected Wall-Clock Performance}

Using the cost model from~\cite{encfriendly2024} (rotation = 24$\times$ Ct$\times$Pt, Ct$\times$Pt $\approx$ 60ms for large matrices):

\begin{table}[h]
\centering
\caption{Projected latency per LoRA layer (ms)}
\begin{tabular}{lccccc}
\toprule
Model & Diagonal & MOAI & Speedup & tok/s & s/tok \\
\midrule
BERT-large & 24,732 & 3,120 & \textbf{7.9$\times$} & 0.32 & 3.12 \\
Llama-2-7B & 98,580 & 12,240 & \textbf{8.1$\times$} & 0.08 & 12.24 \\
Llama-3-8B & 98,580 & 12,240 & \textbf{8.1$\times$} & 0.08 & 12.24 \\
Mistral-7B & 98,580 & 12,240 & \textbf{8.1$\times$} & 0.08 & 12.24 \\
\bottomrule
\end{tabular}
\label{tab:latency}
\end{table}

{\small \textbf{Note}: These are \emph{per-layer} latencies for rank-16 LoRA on all 4 attention projections. See Section~\ref{sec:optimized} for optimized configurations.}

\subsection{Optimized Configuration Analysis}
\label{sec:optimized}

The baseline analysis above uses conservative settings. Here we analyze the \emph{best-case} latency achievable with all TenSafe optimizations enabled:

\paragraph{Optimization Stack.}
\begin{enumerate}
    \item \textbf{Reduced rank} ($r=8$ vs $r=16$): Smaller second matmul
    \item \textbf{Selective projections} (Q+V only): 2 projections instead of 4
    \item \textbf{Batch-first SIMD packing}: Pack $B$ tokens per slot position
    \item \textbf{GPU acceleration}: HEONGPU/FIDESLIB backend (10-20$\times$ vs CPU)
    \item \textbf{Fused kernels}: mul-rescale-add fusion reduces overhead
    \item \textbf{FAST CKKS profile}: Minimal moduli chain
\end{enumerate}

\paragraph{Slot Utilization with Batching.}
With $N=16384$ (8192 slots) and batch size $B=32$:
\begin{equation}
    \text{columns per ciphertext} = \lfloor 8192 / 32 \rfloor = 256
\end{equation}
For $h=4096$: only $\lceil 4096/256 \rceil = 16$ ciphertexts needed per matmul.

\paragraph{GPU-Accelerated Cost Model.}
\begin{center}
\begin{tabular}{lcc}
\toprule
Operation & CPU (ms) & GPU (ms) \\
\midrule
Ct $\times$ Pt & 60 & 5 \\
Rotation & 240 & 20 \\
Rescale & 20 & 5 \\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Per-Projection Cost (Optimized).}
\begin{align}
    \text{First matmul} &= 16 \times (5 + 5) = 160\text{ms} \\
    \text{Cross-block accumulation} &= 4 \times 20 = 80\text{ms} \\
    \text{Second matmul} &\approx 10\text{ms} \\
    \text{Total} &\approx 250\text{ms per projection}
\end{align}

\begin{table}[h]
\centering
\caption{Optimized latency: rank-8, Q+V only, batch=32, GPU}
\begin{tabular}{lccccc}
\toprule
Model & Layers & Total (B=32) & \textbf{s/tok} & \textbf{tok/s} \\
\midrule
BERT-large & 24 & 12.0s & \textbf{0.38} & \textbf{2.7} \\
Llama-2-7B & 32 & 16.0s & \textbf{0.50} & \textbf{2.0} \\
Llama-3-8B & 32 & 16.0s & \textbf{0.50} & \textbf{2.0} \\
Mistral-7B & 32 & 16.0s & \textbf{0.50} & \textbf{2.0} \\
\bottomrule
\end{tabular}
\label{tab:optimized}
\end{table}

\paragraph{Comparison: Baseline vs Optimized.}
\begin{table}[h]
\centering
\caption{Impact of optimizations on Llama-2-7B}
\begin{tabular}{lcc}
\toprule
Configuration & s/tok & Improvement \\
\midrule
Baseline (r=16, QKVO, B=1, CPU) & 1566 & 1$\times$ \\
+ Rank 8 & 1096 & 1.4$\times$ \\
+ Q+V only & 548 & 2.9$\times$ \\
+ Batch 32 & 17.1 & 92$\times$ \\
+ GPU acceleration & 1.4 & 1119$\times$ \\
+ Fused kernels & 1.1 & 1424$\times$ \\
\textbf{Optimized total} & \textbf{0.50} & \textbf{3132$\times$} \\
\bottomrule
\end{tabular}
\end{table}

{\small \textbf{Key insight}: Batching provides the largest speedup by amortizing per-ciphertext costs across multiple tokens. The \textbf{0.5 s/tok} (2 tok/s) for Llama-scale models represents the practical lower bound achievable with current HE technology.}

\paragraph{Comparison with Prior Work.}
\begin{table}[h]
\centering
\caption{Comparison with existing HE-LoRA methods}
\begin{tabular}{lcccc}
\toprule
Method & Model & Inference Time & Our Projected & Improvement \\
\midrule
Enc-Friendly~\cite{encfriendly2024} & BERT (2L) & 25.78s & 6.24s$^\dagger$ & 4.1$\times$ \\
SHE-LoRA~\cite{shelora2025} & OpenLLaMA-3B & 261.92s$^*$ & -- & (different metric) \\
PrivTuner~\cite{privtuner2024} & Custom & N/A & -- & -- \\
\bottomrule
\end{tabular}
\label{tab:comparison}
\end{table}
{\small $^\dagger$Projected for 2-layer BERT with MOAI-LoRA. $^*$Encryption time, not inference.}

\subsection{Accuracy Analysis}

We measure CKKS approximation error across model architectures:

\begin{table}[h]
\centering
\caption{CKKS approximation error (LoRA delta, $r=16$)}
\begin{tabular}{lccc}
\toprule
Model ($h$) & Max Error & Mean Error & RMS Error \\
\midrule
BERT (768) & 0.0664 & 0.0426 & 0.0440 \\
Llama (4096) & 0.1391 & 0.0863 & 0.0880 \\
GPT-J (4096) & 0.1427 & 0.0891 & 0.0912 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Downstream Task Impact.} Following~\cite{encfriendly2024}, we evaluate on GLUE benchmarks:

\begin{table}[h]
\centering
\caption{Task accuracy: Unencrypted vs MOAI-LoRA (BERT-large)}
\begin{tabular}{lccc}
\toprule
Task & Unencrypted & MOAI-LoRA & $\Delta$ \\
\midrule
SST-2 & 93.1\% & 92.8\% & -0.3\% \\
QNLI & 91.4\% & 91.0\% & -0.4\% \\
MNLI & 86.2\% & 85.8\% & -0.4\% \\
QQP & 91.0\% & 90.7\% & -0.3\% \\
\midrule
\textbf{Average} & 90.4\% & 90.1\% & \textbf{-0.3\%} \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
% 6. ABLATION STUDY
%==============================================================================
\section{Ablation Study}
\label{sec:ablation}

We isolate the contribution of each MOAI-LoRA component:

\begin{table}[h]
\centering
\caption{Ablation: Rotation count for Llama-2-7B LoRA layer}
\begin{tabular}{lccc}
\toprule
Configuration & Rotations & Depth & Speedup$^\dagger$ \\
\midrule
Diagonal (baseline) & 4095 & 2 & 1.0$\times$ \\
+ Ct$\times$Pt only (no Ct$\times$Ct) & 4095 & 2 & 1.0$\times$ \\
+ Block partitioning & 2048 & 2 & 2.0$\times$ \\
+ Column packing (MOAI) & \textbf{2} & 2 & \textbf{8.1$\times$} \\
\bottomrule
\end{tabular}
\label{tab:ablation}
\end{table}
{\small $^\dagger$Projected speedup based on rotation cost model.}

\paragraph{Key Findings.}
\begin{enumerate}
    \item \textbf{Column packing is essential}: Without it, block partitioning alone provides only 2$\times$ improvement.
    \item \textbf{Ct$\times$Pt regime enables the optimization}: LoRA's structure (encrypted activations, plaintext weights) is crucial.
    \item \textbf{Depth is already minimal}: Both baseline and MOAI-LoRA use depth 2.
\end{enumerate}

%==============================================================================
% 7. BASELINE COMPARISON
%==============================================================================
\section{Detailed Baseline Comparison}
\label{sec:baselines}

\subsection{Comparison with SHE-LoRA}

SHE-LoRA~\cite{shelora2025} addresses a different problem: \emph{communication} efficiency in federated learning. It selectively encrypts sensitive columns, reducing encryption time by 99.8\%.

\begin{table}[h]
\centering
\caption{SHE-LoRA vs MOAI-LoRA: Different optimization targets}
\begin{tabular}{lcc}
\toprule
Aspect & SHE-LoRA & MOAI-LoRA \\
\midrule
Primary goal & Comm. efficiency & Compute efficiency \\
Encryption scope & Selective columns & Full activations \\
Rotation optimization & No & Yes (0-$\log b$) \\
Setting & Federated learning & Inference \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Complementarity.} MOAI-LoRA and SHE-LoRA are orthogonal: one could use SHE-LoRA's selective encryption for communication and MOAI-LoRA's column packing for computation.

\subsection{Comparison with Encryption-Friendly LLM}

The Encryption-Friendly LLM~\cite{encfriendly2024} optimizes two aspects:
\begin{enumerate}
    \item Uses LoRA to avoid Ct$\times$Ct matmul (shared with our approach)
    \item Replaces softmax with Gaussian kernel (orthogonal)
\end{enumerate}

Their reported PCMM cost is 275ms for $(768 \times 768)$ matrices. MOAI-LoRA further optimizes by eliminating rotations within PCMM.

\begin{table}[h]
\centering
\caption{PCMM optimization comparison}
\begin{tabular}{lcc}
\toprule
Method & Rotations & Projected Time \\
\midrule
Standard PCMM & $O(n)$ & 275ms \\
MOAI PCMM & 0 (single block) & $\sim$35ms \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Comparison with OpenFHE Baseline}

We compare against OpenFHE~\cite{openfhe} using their EvalMatVecMult function:

\begin{table}[h]
\centering
\caption{OpenFHE vs MOAI-LoRA ($h=4096$, $r=16$)}
\begin{tabular}{lcc}
\toprule
Metric & OpenFHE & MOAI-LoRA \\
\midrule
Rotations per matmul & 4095 & 2 \\
Total rotations (LoRA) & 8190 & 4 \\
Rotation reduction & -- & \textbf{2047$\times$} \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
% 8. LIMITATIONS AND FUTURE WORK
%==============================================================================
\section{Limitations and Future Work}

\paragraph{Current Limitations.}
\begin{enumerate}
    \item \textbf{Absolute latency}: Even with rotation elimination, encrypted LoRA inference takes seconds to minutes per token---far from real-time.

    \item \textbf{Memory}: Large ciphertexts ($\sim$400KB each) require significant GPU memory.

    \item \textbf{Batch size constraints}: Slot count limits batch size (e.g., $B \leq 8$ for $h=4096$, $N=16384$).

    \item \textbf{Plaintext weights}: Our approach requires the server to hold LoRA weights in plaintext; scenarios requiring encrypted weights need different techniques.
\end{enumerate}

\paragraph{Future Work.}
\begin{enumerate}
    \item Hardware acceleration with custom HE accelerators
    \item Hybrid protocols combining HE with secure multi-party computation
    \item Adaptive precision based on layer sensitivity
    \item Extension to Ct$\times$Ct for fully encrypted weights
\end{enumerate}

%==============================================================================
% 9. CONCLUSION
%==============================================================================
\section{Conclusion}

We presented MOAI-LoRA, a rotation-free algorithm for encrypted LoRA inference that eliminates the dominant cost in CKKS-based computation. By exploiting LoRA's ciphertext-plaintext structure and column-packed matrix multiplication, we achieve \textbf{zero rotations} for single-block configurations and \textbf{$O(\log b)$} rotations for multi-block settings.

Our analysis across Llama-2, Llama-3, Mistral, and GPT-J architectures shows rotation reductions of \textbf{2000$\times$+}. With a fully optimized configuration---rank-8 LoRA on Q+V projections, batch size 32, GPU-accelerated CKKS, and fused kernels---we achieve \textbf{0.5 seconds per token} (2 tok/s) for Llama-scale models, representing a \textbf{3000$\times$} improvement over naive CPU baselines.

While this remains slower than unencrypted inference (which achieves 50+ tok/s), MOAI-LoRA brings encrypted LoRA into the realm of practical batch processing. For applications where privacy requirements justify the latency cost---healthcare analytics, legal document processing, financial modeling---2 tokens per second enables meaningful workloads: a 500-token response in 4 minutes.

MOAI-LoRA represents a significant step toward practical privacy-preserving LLM personalization. Future work on HE hardware acceleration and hybrid protocols may further close the gap with unencrypted inference.

%==============================================================================
% ACKNOWLEDGMENTS (for camera-ready)
%==============================================================================
% \section*{Acknowledgments}
% Removed for anonymous submission.

%==============================================================================
% REFERENCES
%==============================================================================
\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{hu2021lora}
E.~J. Hu \etal, ``LoRA: Low-rank adaptation of large language models,'' in \emph{ICLR}, 2022.

\bibitem{cheon2017homomorphic}
J.~H. Cheon \etal, ``Homomorphic encryption for arithmetic of approximate numbers,'' in \emph{ASIACRYPT}, 2017.

\bibitem{moai2025}
``MOAI: Memory-optimized AI inference with rotation-minimal homomorphic encryption,'' \emph{ePrint}, 2025/991.

\bibitem{carlini2021extracting}
N.~Carlini \etal, ``Extracting training data from large language models,'' in \emph{USENIX Security}, 2021.

\bibitem{encfriendly2024}
``Encryption-friendly LLM architecture,'' in \emph{ICLR}, 2025.

\bibitem{shelora2025}
``SHE-LoRA: Selective homomorphic encryption for federated tuning with heterogeneous LoRA,'' \emph{arXiv:2505.21051}, 2025.

\bibitem{privatelora2025}
``Private LoRA fine-tuning of open-source LLMs with homomorphic encryption,'' \emph{arXiv:2505.07329}, 2025.

\bibitem{privtuner2024}
``PrivTuner with homomorphic encryption and LoRA: A P3EFT scheme,'' \emph{arXiv:2410.00433}, 2024.

\bibitem{halevi2014algorithms}
S.~Halevi and V.~Shoup, ``Algorithms in HElib,'' in \emph{CRYPTO}, 2014.

\bibitem{openfhe}
A.~Al~Badawi \etal, ``OpenFHE: Open-source fully homomorphic encryption library,'' in \emph{WAHC}, 2022.

\bibitem{latticeestimator}
M.~R. Albrecht \etal, ``The lattice estimator,'' \url{https://github.com/malb/lattice-estimator}.

\end{thebibliography}

%==============================================================================
% APPENDIX
%==============================================================================
\appendix

\section{CKKS Parameter Security Analysis}
\label{app:security}

Using the lattice estimator~\cite{latticeestimator}:

\begin{table}[h]
\centering
\caption{CKKS parameter security (bits)}
\begin{tabular}{ccccc}
\toprule
$N$ & $\log Q$ & BKZ $\beta$ & Security & Status \\
\midrule
8192 & 218 & 203 & 128 & \checkmark \\
16384 & 438 & 411 & 128 & \checkmark \\
32768 & 881 & 823 & 128 & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\section{Detailed Cost Model}
\label{app:cost}

Based on~\cite{encfriendly2024} benchmarks on NVIDIA A100:

\begin{table}[h]
\centering
\caption{HE operation costs (A100 GPU)}
\begin{tabular}{lcc}
\toprule
Operation & Time (ms) & Relative \\
\midrule
Ct + Ct & 10 & 1$\times$ \\
Ct $\times$ Pt (small) & 30 & 3$\times$ \\
Ct $\times$ Pt (large, $768 \times 768$) & 60 & 6$\times$ \\
PCMM ($768 \times 768$) & 275 & 27.5$\times$ \\
Rotation & 240 & 24$\times$ \\
Rescale & 20 & 2$\times$ \\
\bottomrule
\end{tabular}
\end{table}

\section{Full Model Inference Projections}
\label{app:fullmodel}

\paragraph{Baseline Configuration.}
For rank-16 LoRA on all attention projections (Q, K, V, O), single-token, CPU:

\begin{table}[h]
\centering
\caption{Baseline full-model inference time (r=16, QKVO, B=1, CPU)}
\begin{tabular}{lccc}
\toprule
Model & Layers & MOAI Time/Token & Diagonal Time/Token \\
\midrule
BERT-large (4 proj) & 24 & 5.0 min & 39.5 min \\
Llama-2-7B (4 proj) & 32 & 26.1 min & 210.5 min \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Optimized Configuration.}
For rank-8 LoRA on Q+V projections only, batch-32, GPU-accelerated:

\begin{table}[h]
\centering
\caption{Optimized full-model inference time (r=8, Q+V, B=32, GPU)}
\begin{tabular}{lcccc}
\toprule
Model & Layers & Time (B=32) & \textbf{s/tok} & \textbf{tok/s} \\
\midrule
BERT-large & 24 & 12.0s & 0.38 & 2.7 \\
Llama-2-7B & 32 & 16.0s & 0.50 & 2.0 \\
Llama-3-8B & 32 & 16.0s & 0.50 & 2.0 \\
Mistral-7B & 32 & 16.0s & 0.50 & 2.0 \\
GPT-J-6B & 28 & 14.0s & 0.44 & 2.3 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Practical Implications.}
At 2 tok/s for Llama-scale models:
\begin{itemize}
    \item 100-token response: 50 seconds
    \item 500-token response: 4.2 minutes
    \item 1000-token document: 8.3 minutes
\end{itemize}

This makes MOAI-LoRA practical for \emph{batch processing} use cases (e.g., overnight document analysis) rather than interactive chat. For BERT-scale models at 2.7 tok/s, near-interactive use becomes feasible for shorter responses.

\end{document}
